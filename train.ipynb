{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PJBU4ZtznlRr"
      },
      "source": [
        "# Train Middle Earth Text Gen"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwwslWfPw3Xe"
      },
      "source": [
        "## Clone the Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9C4mS1EnlRr"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/brotSchimmelt/MiddleEarthTextGen.git\n",
        "\n",
        "import os\n",
        "os.chdir(\"MiddleEarthTextGen\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F6nE4HMLw7uV"
      },
      "source": [
        "## Load the Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZmhk3lonpfe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "upload = files.upload()\n",
        "file_name = list(upload.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43nCG_ttn_2e"
      },
      "outputs": [],
      "source": [
        "with open(file_name, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'{file_name} is {len(text):,} characters long with a vocabulary size of {vocab_size}.\\n')\n",
        "print(f'Vocabulary: {repr(\"\".join(vocab))}\\n')\n",
        "print(f'First 500 character sequence:\\n{text[:500]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE5GPKqSvDg4"
      },
      "outputs": [],
      "source": [
        "# create a simple character tokenizer\n",
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_string = \"hello there\"\n",
        "assert decode(encode(test_string)) == test_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train / Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0.9 / 0.1 split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Context Length and Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "context_length = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        return logits\n",
        "\n",
        "bigram_model = BigramLanguageModel(vocab_size)\n",
        "out = bigram_model(xb, yb)\n",
        "print(out.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lotrGPT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0cd598e4c2cb8528f621442517540a5cc7ae47a2f41a054f8ca4c96ef70435cf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
